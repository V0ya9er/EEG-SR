#!/usr/bin/env python
"""
LOSO äº¤å‰éªŒè¯ç»“æœèšåˆåˆ†æ

åŠŸèƒ½:
- æ”¶é›†å„æŠ˜å®éªŒç»“æœï¼ˆä»æ–°çš„ outputs/ ç›®å½•ï¼‰
- è®¡ç®—æ¯ä¸ªè¢«è¯•çš„æ€§èƒ½
- è®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
- ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
- å¯¼å‡º CSV

ä½¿ç”¨æ–¹å¼:
    python scripts/loso_analyze.py --results-dir ./outputs
    python scripts/loso_analyze.py --results-dir ./outputs --output ./results/summary.csv
"""
import os
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, List, Optional, Tuple
import re

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•
_project_root = Path(__file__).parent.parent
sys.path.insert(0, str(_project_root))

try:
    import pandas as pd
    import numpy as np
except ImportError:
    print("è¯·å®‰è£… pandas å’Œ numpy: pip install pandas numpy")
    sys.exit(1)


def parse_folder_name(folder_name: str) -> Optional[Dict]:
    """
    è§£æè¯­ä¹‰åŒ–æ–‡ä»¶å¤¹å
    
    æ ¼å¼: {dataset}_{model}_{mechanism}_{noise}_fold{id}_D{intensity}
    ä¾‹å¦‚: bci2a_eegnet_add_gauss_fold1_D0.5
    
    Returns:
        è§£æåçš„å­—å…¸ï¼Œæˆ– Noneï¼ˆè§£æå¤±è´¥ï¼‰
    """
    # æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
    pattern = r'^(\w+)_(\w+)_(\w+)_(\w+)_fold(\d+)_D([\d.]+)$'
    match = re.match(pattern, folder_name)
    
    if match:
        return {
            "dataset": match.group(1),
            "model": match.group(2),
            "mechanism": match.group(3),
            "noise": match.group(4),
            "fold_id": int(match.group(5)),
            "intensity": float(match.group(6)),
        }
    return None


def find_experiment_results(results_dir: Path) -> List[Dict]:
    """
    æœç´¢ç»“æœç›®å½•ï¼Œæ”¶é›†æ‰€æœ‰å®éªŒçš„ training_info.json
    
    æœç´¢ä¼˜å…ˆçº§:
    1. outputs/ ç›®å½•ï¼ˆæ–°çš„è¯­ä¹‰åŒ–å‘½åï¼‰
    2. outputs/multirun/ ç›®å½•ï¼ˆHydra multirun è¾“å‡ºï¼‰
    3. lightning_logs/ ç›®å½•ï¼ˆæ—§æ ¼å¼ï¼Œå‘åå…¼å®¹ï¼‰
    
    Returns:
        å®éªŒç»“æœåˆ—è¡¨
    """
    results = []
    
    # 1. æœç´¢ outputs ç›®å½•ï¼ˆæ–°æ ¼å¼ï¼‰
    outputs_dir = results_dir / "outputs" if (results_dir / "outputs").exists() else results_dir
    
    for exp_dir in sorted(outputs_dir.iterdir()):
        if not exp_dir.is_dir():
            continue
        
        # è·³è¿‡ multirun ç›®å½•ï¼Œåé¢å•ç‹¬å¤„ç†
        if exp_dir.name == "multirun":
            continue
        
        info_file = exp_dir / "training_info.json"
        if info_file.exists():
            with open(info_file, 'r', encoding='utf-8') as f:
                info = json.load(f)
            info["result_path"] = str(exp_dir)
            info["folder_name"] = exp_dir.name
            results.append(info)
        else:
            # å°è¯•ä»æ–‡ä»¶å¤¹åè§£æ
            parsed = parse_folder_name(exp_dir.name)
            if parsed:
                parsed["result_path"] = str(exp_dir)
                parsed["folder_name"] = exp_dir.name
                results.append(parsed)
    
    # 2. æœç´¢ multirun ç›®å½•
    multirun_dir = outputs_dir / "multirun"
    if multirun_dir.exists():
        for exp_dir in sorted(multirun_dir.iterdir()):
            if exp_dir.is_dir():
                info_file = exp_dir / "training_info.json"
                if info_file.exists():
                    with open(info_file, 'r', encoding='utf-8') as f:
                        info = json.load(f)
                    info["result_path"] = str(exp_dir)
                    info["folder_name"] = exp_dir.name
                    results.append(info)
    
    # 3. å‘åå…¼å®¹ï¼šæœç´¢ lightning_logs ç›®å½•
    lightning_logs = results_dir / "lightning_logs"
    if lightning_logs.exists():
        for version_dir in sorted(lightning_logs.iterdir()):
            if version_dir.is_dir() and version_dir.name.startswith("version_"):
                info_file = version_dir / "training_info.json"
                if info_file.exists():
                    with open(info_file, 'r', encoding='utf-8') as f:
                        info = json.load(f)
                    info["result_path"] = str(version_dir)
                    info["folder_name"] = version_dir.name
                    results.append(info)
    
    return results


def read_metrics(result_path: Path) -> Tuple[float, float, float]:
    """
    è¯»å–å®éªŒæŒ‡æ ‡
    
    å°è¯•ä»ä»¥ä¸‹ä½ç½®è¯»å–:
    1. training_info.json ä¸­çš„ final_metrics
    2. metrics.csvï¼ˆLightning æ ¼å¼ï¼‰
    
    Returns:
        (accuracy, f1, kappa) æˆ– (nan, nan, nan)
    """
    # å°è¯•ä» training_info.json è¯»å–
    info_file = result_path / "training_info.json"
    if info_file.exists():
        with open(info_file, 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        if "final_metrics" in info:
            fm = info["final_metrics"]
            return (
                fm.get("test_acc", float('nan')),
                fm.get("test_f1", float('nan')),
                fm.get("test_kappa", float('nan'))
            )
    
    # å°è¯•ä» metrics.csv è¯»å–
    metrics_file = result_path / "metrics.csv"
    if metrics_file.exists():
        try:
            df = pd.read_csv(metrics_file)
            # æŸ¥æ‰¾æµ‹è¯•æŒ‡æ ‡åˆ—
            for acc_col in ["test_acc", "test/acc", "test_accuracy"]:
                if acc_col in df.columns:
                    test_rows = df.dropna(subset=[acc_col])
                    if not test_rows.empty:
                        last_row = test_rows.iloc[-1]
                        return (
                            last_row.get(acc_col, float('nan')),
                            last_row.get("test_f1", last_row.get("test/f1", float('nan'))),
                            last_row.get("test_kappa", last_row.get("test/kappa", float('nan')))
                        )
        except Exception:
            pass
    
    # æ£€æŸ¥å­ç›®å½•ï¼ˆå¦‚ lightning_logs/version_0ï¼‰
    for subdir in result_path.iterdir():
        if subdir.is_dir():
            sub_metrics = subdir / "metrics.csv"
            if sub_metrics.exists():
                try:
                    df = pd.read_csv(sub_metrics)
                    for acc_col in ["test_acc", "test/acc", "test_accuracy"]:
                        if acc_col in df.columns:
                            test_rows = df.dropna(subset=[acc_col])
                            if not test_rows.empty:
                                last_row = test_rows.iloc[-1]
                                return (
                                    last_row.get(acc_col, float('nan')),
                                    last_row.get("test_f1", last_row.get("test/f1", float('nan'))),
                                    last_row.get("test_kappa", last_row.get("test/kappa", float('nan')))
                                )
                except Exception:
                    pass
    
    return (float('nan'), float('nan'), float('nan'))


def aggregate_fold_results(results: List[Dict]) -> pd.DataFrame:
    """
    èšåˆå„æŠ˜ç»“æœ
    """
    rows = []
    
    for result in results:
        result_path = Path(result.get("result_path", ""))
        accuracy, f1, kappa = read_metrics(result_path)
        
        rows.append({
            "dataset": result.get("dataset_name", result.get("dataset", "unknown")),
            "model": result.get("model_name", result.get("model", "unknown")),
            "mechanism": result.get("mechanism_name", result.get("mechanism", "unknown")),
            "noise": result.get("noise_name", result.get("noise", "unknown")),
            "intensity": result.get("intensity", 0),
            "fold_id": result.get("fold_id", 0),
            "n_folds": result.get("n_folds", 0),
            "test_subjects": str(result.get("test_subjects", [])),
            "accuracy": accuracy,
            "f1": f1,
            "kappa": kappa,
            "folder": result.get("folder_name", ""),
            "result_path": result.get("result_path", "")
        })
    
    return pd.DataFrame(rows)


def compute_summary_statistics(df: pd.DataFrame) -> pd.DataFrame:
    """
    æŒ‰å®éªŒé…ç½®åˆ†ç»„ï¼Œè®¡ç®—å¹³å‡å€¼å’Œæ ‡å‡†å·®
    """
    group_cols = ["dataset", "model", "mechanism", "noise", "intensity"]
    
    summary_rows = []
    for name, group in df.groupby(group_cols, dropna=False):
        dataset, model, mechanism, noise, intensity = name
        
        summary_rows.append({
            "dataset": dataset,
            "model": model,
            "mechanism": mechanism,
            "noise": noise,
            "intensity": intensity,
            "n_folds_actual": len(group),
            "accuracy_mean": group["accuracy"].mean(),
            "accuracy_std": group["accuracy"].std(),
            "f1_mean": group["f1"].mean(),
            "f1_std": group["f1"].std(),
            "kappa_mean": group["kappa"].mean(),
            "kappa_std": group["kappa"].std()
        })
    
    return pd.DataFrame(summary_rows)


def print_subject_performance(df: pd.DataFrame):
    """æ‰“å°æ¯ä¸ªè¢«è¯•/æŠ˜çš„æ€§èƒ½"""
    print("\n" + "="*70)
    print("ğŸ“Š æ¯ä¸ªæŠ˜çš„æµ‹è¯•æ€§èƒ½")
    print("="*70)
    
    # æŒ‰é…ç½®åˆ†ç»„
    for (dataset, model, mech, noise, intensity), group in df.groupby(
        ["dataset", "model", "mechanism", "noise", "intensity"], dropna=False
    ):
        print(f"\nğŸ”¬ {dataset} | {model} | {mech} | {noise} | D={intensity}")
        print("-" * 60)
        
        for _, row in group.iterrows():
            acc_str = f"{row['accuracy']:.4f}" if not pd.isna(row['accuracy']) else "N/A"
            f1_str = f"{row['f1']:.4f}" if not pd.isna(row['f1']) else "N/A"
            kappa_str = f"{row['kappa']:.4f}" if not pd.isna(row['kappa']) else "N/A"
            
            print(f"  Fold {row['fold_id']}: æµ‹è¯•è¢«è¯• {row['test_subjects']}")
            print(f"    Accuracy: {acc_str}  |  F1: {f1_str}  |  Kappa: {kappa_str}")


def print_summary(summary_df: pd.DataFrame):
    """æ‰“å°æ±‡æ€»ç»Ÿè®¡"""
    print("\n" + "="*70)
    print("ğŸ“ˆ å®éªŒæ±‡æ€» (å¹³å‡ Â± æ ‡å‡†å·®)")
    print("="*70)
    
    for _, row in summary_df.iterrows():
        config = f"{row['dataset']}_{row['model']}_{row['mechanism']}_{row['noise']}_D{row['intensity']}"
        
        acc_str = f"{row['accuracy_mean']:.4f} Â± {row['accuracy_std']:.4f}" if not pd.isna(row['accuracy_mean']) else "N/A"
        f1_str = f"{row['f1_mean']:.4f} Â± {row['f1_std']:.4f}" if not pd.isna(row['f1_mean']) else "N/A"
        kappa_str = f"{row['kappa_mean']:.4f} Â± {row['kappa_std']:.4f}" if not pd.isna(row['kappa_mean']) else "N/A"
        
        print(f"\nğŸ“Œ {config}")
        print(f"   Accuracy: {acc_str}")
        print(f"   F1 Score: {f1_str}")
        print(f"   Kappa:    {kappa_str}")
        print(f"   (åŸºäº {row['n_folds_actual']} æŠ˜)")


def find_optimal_config(summary_df: pd.DataFrame) -> Optional[Dict]:
    """æ‰¾åˆ°æœ€ä¼˜é…ç½®"""
    if summary_df.empty:
        return None
    
    # è¿‡æ»¤æ‰æ²¡æœ‰æœ‰æ•ˆå‡†ç¡®ç‡çš„è¡Œ
    valid_df = summary_df.dropna(subset=["accuracy_mean"])
    if valid_df.empty:
        return None
    
    best_idx = valid_df["accuracy_mean"].idxmax()
    best = valid_df.loc[best_idx]
    
    return {
        "dataset": best["dataset"],
        "model": best["model"],
        "mechanism": best["mechanism"],
        "noise": best["noise"],
        "intensity": best["intensity"],
        "accuracy": f"{best['accuracy_mean']:.4f} Â± {best['accuracy_std']:.4f}",
        "f1": f"{best['f1_mean']:.4f} Â± {best['f1_std']:.4f}",
        "kappa": f"{best['kappa_mean']:.4f} Â± {best['kappa_std']:.4f}"
    }


def main():
    parser = argparse.ArgumentParser(description="LOSO ç»“æœèšåˆåˆ†æ")
    parser.add_argument("--results-dir", type=str, default=".",
                        help="ç»“æœç›®å½• (é»˜è®¤: å½“å‰ç›®å½•)")
    parser.add_argument("--output", "-o", type=str, default=None,
                        help="è¾“å‡ºè¯¦ç»†ç»“æœ CSV æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--summary-output", "-s", type=str, default=None,
                        help="è¾“å‡ºæ±‡æ€» CSV æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--verbose", "-v", action="store_true",
                        help="æ˜¾ç¤ºæ¯ä¸ªæŠ˜çš„è¯¦ç»†ä¿¡æ¯")
    parser.add_argument("--format", type=str, choices=["table", "json"], default="table",
                        help="è¾“å‡ºæ ¼å¼")
    
    args = parser.parse_args()
    
    results_dir = Path(args.results_dir)
    
    # æ”¶é›†ç»“æœ
    print(f"ğŸ” æœç´¢ç»“æœç›®å½•: {results_dir.absolute()}")
    results = find_experiment_results(results_dir)
    print(f"ğŸ“‚ æ‰¾åˆ° {len(results)} ä¸ªå®éªŒç»“æœ")
    
    if not results:
        print("\nâŒ æœªæ‰¾åˆ°ä»»ä½•å®éªŒç»“æœ")
        print("\næç¤º: ç¡®ä¿ç»“æœç›®å½•åŒ…å«ä»¥ä¸‹ä¹‹ä¸€:")
        print("  - outputs/{dataset}_{model}_{mech}_{noise}_fold{id}_D{intensity}/training_info.json")
        print("  - lightning_logs/version_X/training_info.json")
        return
    
    # èšåˆç»“æœ
    df = aggregate_fold_results(results)
    
    # è®¡ç®—æ±‡æ€»ç»Ÿè®¡
    summary_df = compute_summary_statistics(df)
    
    # è¾“å‡º
    if args.format == "json":
        print(json.dumps({
            "experiments": df.to_dict(orient="records"),
            "summary": summary_df.to_dict(orient="records")
        }, indent=2, ensure_ascii=False))
    else:
        if args.verbose:
            print_subject_performance(df)
        
        print_summary(summary_df)
        
        # æ‰¾åˆ°æœ€ä¼˜é…ç½®
        optimal = find_optimal_config(summary_df)
        if optimal:
            print("\n" + "="*70)
            print("ğŸ† æœ€ä¼˜é…ç½®")
            print("="*70)
            for k, v in optimal.items():
                print(f"   {k}: {v}")
    
    # ä¿å­˜ç»“æœ
    if args.output:
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        df.to_csv(output_path, index=False)
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    
    if args.summary_output:
        summary_path = Path(args.summary_output)
        summary_path.parent.mkdir(parents=True, exist_ok=True)
        summary_df.to_csv(summary_path, index=False)
        print(f"ğŸ’¾ æ±‡æ€»ç»Ÿè®¡å·²ä¿å­˜åˆ°: {summary_path}")


if __name__ == "__main__":
    main()